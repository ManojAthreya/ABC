{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardiac Arrythmia ML\n",
    "\n",
    "## Machine Learning Engineer Nanodegree\n",
    "\n",
    "### Capstone Project\n",
    "\n",
    "In this project we will try to classify Phonocardiogram (PCG) or heartbeat recordings as \"normal\" or \"abnormal\" to identify patients who would require further diagnosis. The basic idea is to convert each heart sound recording(wav file) to a spectrogram image and train a Convolutional Neural Network. Then given a new PCG recording, we will be able to classify it as normal or abnormal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries\n",
    "\n",
    "Here we are importing all the libraries needed for the project. We will be using Keras(Tensorflow backend) to build our model since it is a high-level neural networks API written in Python and very easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Reshape, Lambda\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the dataset\n",
    "\n",
    "The heartbeat recording can be downloaded from here. The dataset contains about 3500 recording but we will only use about 1000 due to memory and computation constraints. After converting the .wav recordings into spectrogram images, the training dataset has 800 images (of which 400 belong to abnormal and 600 to normal class) and test set contains around 225 images(80 of abnormal and rest belong to normal class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting .wav files (heartbeat recording) to spectrograms\n",
    "\n",
    "Our model (Convolutional Neural Network) takes images as input. So we first need to convert the recordings into spectrogram images. This is taken care by 'convert_to_spectrogram.py' which is present in the same repostiory as this notebook. Hence before experimenting with this notebook, it is required to run  'convert_to_spectrogram.py'. It will automatically put the images in the following folder structure as shown below. (Note : It is manually required to create empty folders according to structure). The techniques inolved to convert recordings into spectrogram images are discussed in the capstone report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure\n",
    "\n",
    "data/\n",
    "\n",
    "    train/\n",
    "        normal/ ### 600 pictures\n",
    "            a001.jpg\n",
    "            a002.jpg\n",
    "            ...\n",
    "        abnormal/ ### 400 pictures\n",
    "            a101.jpg\n",
    "            a102.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        normal/ ### 143 pictures\n",
    "            f001.jpg\n",
    "            f002.jpg\n",
    "            ...\n",
    "        abnormal/ ### 82 pictures\n",
    "            f101.jpg\n",
    "            f102.jpg\n",
    "            ...\n",
    "            \n",
    "To process images we require the Pillow framework. You can install it using pip3 install Pillow.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining image dimensions and training and testing set folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension for our images\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# Directories containing image\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset and a brief introduction to Spectrograms\n",
    "\n",
    "Here lets see what the spectrogram images after converting from .wav recording look like. \n",
    "\n",
    "Spectrograms are a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a waveform. They are also used to see how energy levels vary over time. They are basically two-dimensional graphs, with a third dimension represented by colors. Time runs from left (oldest) to right (youngest) along the horizontal axis. The vertical axis represents frequency, which can also be thought of as pitch or tone, with the lowest frequencies at the bottom and the highest frequencies at the top.  The amplitude (or energy or “loudness”) of a particular frequency at a particular time is represented by the third dimension, color, with dark blues corresponding to low amplitudes and brighter colors up through red corresponding to progressively stronger (or louder) amplitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples spectrograms from the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "listOfImageNames = ['data/train/normal/a0408.png',\n",
    "                    'data/validation/normal/a0009.png',\n",
    "                   'data/train/abnormal/a0117.png',\n",
    "                   'data/validation/abnormal/a0001.png']\n",
    "\n",
    "\n",
    "for imageName in listOfImageNames:\n",
    "    display(Image(filename=imageName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting number of images in training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nor_train_dir = 'data/train/normal/'\n",
    "nor_valid_dir = 'data/validation/normal/'\n",
    "abnor_train_dir = 'data/train/abnormal/'\n",
    "abnor_valid_dir = 'data/validation/abnormal/'\n",
    "\n",
    "nor_train = len(next(os.walk(nor_train_dir))[2])\n",
    "nor_valid = len(next(os.walk(nor_valid_dir))[2])\n",
    "abnor_train = len(next(os.walk(abnor_train_dir))[2])\n",
    "abnor_valid = len(next(os.walk(abnor_valid_dir))[2])\n",
    "\n",
    "## We subtract 1 since every directory has a hidden file .DS_Store\n",
    "print ('Number of samples in training set (normal): {}'.format((nor_train)-1))\n",
    "print ('Number of samples in validation set (normal): {}'.format((nor_valid)-1))\n",
    "print ('Number of samples in training set (abnormal): {}'.format((abnor_train)-1))\n",
    "print ('Number of samples in validation set (abnormal): {}'.format((abnor_valid)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the samples using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "n_groups = 2\n",
    "\n",
    "train_samples = (nor_train, nor_valid)\n",
    "validation_samples = (abnor_train, abnor_valid)\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.3\n",
    "opacity = 0.6\n",
    " \n",
    "rects1 = plt.bar(index, train_samples, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 label='normal')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, validation_samples, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='g',\n",
    "                 label='abnormal')\n",
    " \n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Samples and Classes')\n",
    "plt.xticks(index + bar_width, ('Train', 'Validation'))\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "One way to get around a lack of data as in our case (since we are only working with a subset of original dataset) is to augment the data. In terms of images, this means increasing the number of images in the dataset. There are many ways to augment data. For images this could be done by rotating the original image, changing lighting conditions, cropping it differently, so for one image we can generate different sub-samples. This way you can reduce overfitting and allow better generalization capability for our network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model we would apply four data augmentation techniques as described below :\n",
    "- Normalize pixel values between 0 and 1. This can be done by setting the 'rescale' attribute to 1/.255\n",
    "- Appling [shear transformations](https://en.wikipedia.org/wiki/Shear_mapping). Can be done with the 'shear_range' attribute\n",
    "- Zooming inside images uding 'zoom_range' attribute\n",
    "- Flipping half of the images horizontally. This is done by setting the 'horizontal_flip' attribute to 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To generate batches of images, Keras provides us with a handy 'ImageDataGenerator' class which takes in the above mentioned attributes and provides us with real-time data augmentation. The data will be looped over (in batches) indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True\n",
    "        )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'ImageDataGenerator' class has the method 'flow_from_directory'.It takes the path to a directory, and generates batches of augmented data and yields batches in an infinite loop. It further provides us with the number of images in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have batches of images available to feed into the network we can now implement and train our model over these images. But before training our network we must define metrics for us to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation Metrics help us to observe how well the model generalises on unseen data i.e to judge the performance of your model. For this project we will use the following metrics: \n",
    "- Accuracy \n",
    "- Precision \n",
    "- Recall\n",
    "- f-Beta score\n",
    "\n",
    "Since Keras only provides us with accuracy, we need to write our own custom metrics that can be passed at the compilation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "It is always better if we are able to see visually how well our model is performing. Hence below we define simple visualisation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def results(history):\n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['acc']); plt.plot(history.history['val_acc']);\n",
    "    plt.title('model accuracy'); plt.ylabel('accuracy');\n",
    "    plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss']); plt.plot(history.history['val_loss']);\n",
    "    plt.title('model loss'); plt.ylabel('loss');\n",
    "    plt.xlabel('epoch'); plt.legend(['train', 'valid'], loc='upper left');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and gives  an output. Convolutional Neural Networks take advantage of the fact that the input consists of images (i.e pixels closer together are more meaningful) and this allows us to encode certain properties into the architecture. This means that the forward function is more efficient to implement and hence vastly reduces the amount of parameters in the network. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth.\n",
    "\n",
    "There are 3 types of layers present in Convolutional Neural Networks :\n",
    "- Convolutional Layers: Convolution is a mathematical operation which is used to filter signals and find patterns in signals etc. In a convolutional layer, all neurons apply convolution operation to the inputs. The convolution layer comprises of a set of independent filters. Each filter is independently convolved with the image and we end up with n feature maps.\n",
    "- Pooling Layers: This is mostly applied immediately after the convolutional layer to reduce the spatial size(only width and height, not depth). This reduces the number of parameters, hence computation is reduced. Also, less number of parameters avoid overfitting. The most common form of pooling is Max pooling where we take a filter of size n*n and apply the maximum operation over the n*n sized part of the image.\n",
    "- Fully Connected Layer: This layer just computes the matrix multiplication of inputs recieved from neurons in the previous layer followed by bias offset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "We will first try a very small CNN with few layers and few filters per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',precision,recall,fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 3\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples) ## [Loss,\n",
    "                                                                      ##  Accuracy,\n",
    "                                                                      ##  Precision,\n",
    "                                                                      ##  Recall\n",
    "                                                                      ##  F-BetaScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a more deeper CNN with more number of layers and number of neurons in each layer. We also use techniques to reduce overfitting if any with dropout and batch normalisation. All these techniques have been discussed in detail in the report for this capstone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "We now implement a deeper CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(img_width,img_height,3)))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(BatchNormalization())          \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))          \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',precision,recall,fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 3\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples) ## [Loss,\n",
    "                                                                      ##  Accuracy,\n",
    "                                                                      ##  Precision,\n",
    "                                                                      ##  Recall\n",
    "                                                                      ##  F-BetaScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning and using pre-trained models\n",
    "\n",
    "Transfer Learning allows us to use pre-trained models trained on datasets with millions of images such as COCO, Imagenet etc. This means instead of building a model from scratch to solve a similar dataset, we can use the model trained on some other dataset as a starting point. Hence by using pre-trained models which have been previously trained on large datasets, we can directly use the weights and architecture obtained and apply the learning on our problem statement and allow us to reach a better accuracy than any method. You can read more about it [here](https://medium.com/@galen.ballew/transferlearning-b65772083b47)\n",
    "\n",
    "For our problem, we are going to the VGG16 model which is available in Keras already. It is Deep convolutional network for object recognition developed and trained by Oxford's renowned Visual Geometry Group (VGG), which achieved very [good performance](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 Model\n",
    "\n",
    "Here we implement the VGG16 network. We only use the network upto the fully connected network and remove all subseuent layers. The weights file for VGG16 is available [here](https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view)\n",
    "\n",
    "!['VGG16 Architecture'](vgg16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = Sequential()\n",
    "model_vgg.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\n",
    "\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model_vgg.add(ZeroPadding2D((1, 1)))\n",
    "model_vgg.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model_vgg.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must now load the weights (vgg16_weights.h5) since the structure of our model is not exactly the same as the one used when training weights. If it was, we could have just used the in-built mehtod as model.load_weights( )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "import h5py\n",
    "f = h5py.File('vgg16_weights.h5')\n",
    "\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model_vgg.layers):\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    \n",
    "    if (len(weights)>0):\n",
    "        #weights[0].shape = weight_shape\n",
    "        weights[0] = np.transpose(weights[0], (2, 3, 1, 0))    \n",
    "    model_vgg.layers[k].set_weights(weights)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the batches for VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_bottleneck = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "validation_generator_bottleneck = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the ourput of our VGG16 model since it takes a while to train and it not efficient to run it everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(train_generator_bottleneck, nb_train_samples)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_validation = model_vgg.predict_generator(validation_generator_bottleneck, nb_validation_samples)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bottleneck_features_train.shape)\n",
    "print (bottleneck_features_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading VGG16's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train.npy', 'rb'))\n",
    "train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG16 was trained for more than 100 classes. But here we are only trying to classify into normal or abnormal. So we now add our own custom fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model_top.add(Dense(1024, activation='relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',precision,recall,fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_top.fit(train_data,\n",
    "                        train_labels,\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save_weights('models/1000-samples.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.load_weights('models/1000-samples.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating our model on test set and visualising the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.evaluate(validation_data, validation_labels) ## [Loss,\n",
    "                                                       ##  Accuracy,\n",
    "                                                       ##  Precision,\n",
    "                                                       ##  Recall\n",
    "                                                       ##  F-BetaScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4\n",
    "\n",
    "Based on VGG16 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just for experimental purposes, I train the VGG16 full architecture with my dataset to see if there are any improvement in validation accuracy and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(img_width,img_height,3)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= VGG_16()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',precision,recall,fbeta_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 3\n",
    "nb_train_samples = 1000\n",
    "nb_validation_samples = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples) ## [Loss,\n",
    "                                                                      ##  Accuracy,\n",
    "                                                                      ##  Precision,\n",
    "                                                                      ##  Recall\n",
    "                                                                      ##  F-BetaScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
